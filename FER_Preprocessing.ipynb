{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition: Facial Expression Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>85 84 90 121 101 102 133 153 153 169 177 189 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>255 254 255 254 254 179 122 107 95 124 149 150...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
       "5        2  55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...  Training\n",
       "6        4  20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...  Training\n",
       "7        3  77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...  Training\n",
       "8        3  85 84 90 121 101 102 133 153 153 169 177 189 1...  Training\n",
       "9        2  255 254 255 254 254 179 122 107 95 124 149 150...  Training"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe total number of rows in dataset\u001b[0m\n",
      "\n",
      "emotion    35887\n",
      "pixels     35887\n",
      "Usage      35887\n",
      "dtype: int64\n",
      "\n",
      "\u001b[1mAll the keywords with their starting index in 'Usage' column\n",
      "\u001b[0m\n",
      "0           Training\n",
      "28709     PublicTest\n",
      "32298    PrivateTest\n",
      "Name: Usage, dtype: object\n",
      "\n",
      "There are 7 Labels in total for each emotion\n",
      "[0 2 4 6 3 5 1]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "def load_raw_data():\n",
    "    \n",
    "    data = pd.read_csv(\"fer2013.csv\")\n",
    "\n",
    "    usage_ix = data.drop_duplicates('Usage')['Usage']\n",
    "    usage_ix = usage_ix.index\n",
    "    n_classes = len(data.drop_duplicates('emotion')['emotion'].values)\n",
    "    return data, usage_ix, n_classes\n",
    "\n",
    "#::::::::TEST:::::::#\n",
    "data, usage_ix, n_classes = load_raw_data()\n",
    "display(data.head(n=10))\n",
    "print (\"\\033[1mThe total number of rows in dataset\\033[0m\\n\")\n",
    "print (data.count())\n",
    "print (\"\\n\\033[1mAll the keywords with their starting index in 'Usage' column\\n\\033[0m\")\n",
    "print (data.drop_duplicates('Usage')['Usage'])\n",
    "\n",
    "print (\"\\nThere are\", n_classes,\"Labels in total for each emotion\")\n",
    "print (data.drop_duplicates('emotion')['emotion'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'training_batch_1.csv' is found\n",
      "File 'training_batch_2.csv' is found\n",
      "File 'training_batch_3.csv' is found\n",
      "File 'training_batch_4.csv' is found\n",
      "File 'training_batch_5.csv' is found\n",
      "File 'training_batch_6.csv' is found\n",
      "File 'public_test.csv' is found\n",
      "File 'private_test.csv' is found\n"
     ]
    }
   ],
   "source": [
    "def dataset_clf(data, usage_ix):\n",
    "    \"\"\"Classify the dataset based on the keyword in 'Usage' column, and divide training data into batches.\n",
    "       Finally, save all sub-dataset into csv file.\n",
    "    \n",
    "    Args:\n",
    "       data: DataFrame of raw dataset\n",
    "       usage_ix: A list of index of unique keywords in 'Usage' column\n",
    "    \"\"\"\n",
    "    ix = 0\n",
    "    n_batches = 6    # tunable\n",
    "    batch_size = 5000    # tunable\n",
    "    \n",
    "    # check or save training batches\n",
    "    for batch_i in range(1,n_batches+1):\n",
    "        \n",
    "        batch_file = Path(\"C:/Users/xiaow/Desktop/fer2013/training_batch_\"+str(batch_i)+\".csv\")\n",
    "        if batch_file.is_file():\n",
    "            print(\"File 'training_batch_\"+str(batch_i)+\".csv' is found\")\n",
    "            ix += batch_size\n",
    "            \n",
    "        else:\n",
    "    \n",
    "            if (usage_ix[1]-1)-ix < batch_size:\n",
    "                data.loc[ix : usage_ix[1]-1].to_csv(\"training_batch_\"+str(batch_i)+\".csv\")\n",
    "                print (\"File 'training_batch_\"+str(batch_i)+\".csv' saved successfully\")\n",
    " \n",
    "            else:\n",
    "                data.loc[usage_ix[0] + ix : ix + (batch_size-1)].to_csv(\"training_batch_\"+str(batch_i)+\".csv\")\n",
    "                print (\"File 'training_batch_\"+str(batch_i)+\".csv' saved successfully\")\n",
    "            \n",
    "            ix += batch_size\n",
    "    \n",
    "    # check or save public test\n",
    "    batch_file = Path(\"C:/Users/xiaow/Desktop/fer2013/public_test.csv\")\n",
    "    if batch_file.is_file():\n",
    "        print(\"File 'public_test.csv' is found\")\n",
    "    else:\n",
    "        data.loc[usage_ix[1]:usage_ix[2]-1].to_csv(\"public_test.csv\")    # save public testing data\n",
    "        print (\"File 'public_test.csv' saved successfully\")\n",
    "\n",
    "    # check or save private test\n",
    "    batch_file = Path(\"C:/Users/xiaow/Desktop/fer2013/private_test.csv\")\n",
    "    if batch_file.is_file():\n",
    "        print(\"File 'private_test.csv' is found\")\n",
    "    else:\n",
    "        data.loc[usage_ix[2]:].to_csv(\"private_test.csv\")    # save private testing data\n",
    "        print (\"File 'private_test.csv' saved successfully\")\n",
    "\n",
    "    return n_batches, batch_size\n",
    "\n",
    "#::::::::TEST:::::::#\n",
    "n_batches, batch_size = dataset_clf(data, usage_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overview of loaded batch:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    5000\n",
       "emotion       5000\n",
       "pixels        5000\n",
       "Usage         5000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_batches(batch_num):\n",
    "    \n",
    "    data = pd.read_csv(\"training_batch_\"+str(batch_num)+\".csv\")\n",
    "            \n",
    "    return data\n",
    "        \n",
    "#::::::::TEST:::::::#\n",
    "batch_num = 1    # tunable\n",
    "batch_data = load_batches(batch_num)\n",
    "print(\"The overview of loaded batch:\")\n",
    "batch_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image_array visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if all training images exist...\n",
      "Checking finished! All images are now available in the path ./Images/\n"
     ]
    }
   ],
   "source": [
    "def training_image_visual(n_batches, batch_size):\n",
    "    \n",
    "    print(\"Checking if all training images exist...\")\n",
    "    for batch_i in range(1,n_batches+1):\n",
    "        \n",
    "        dir_path = \"./Images/Batch\"+str(batch_i)   # check if the directory exists\n",
    "        if os.path.isdir(dir_path):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(dir_path)\n",
    "        \n",
    "        num_files = len(os.listdir(dir_path))    # check if all images exist\n",
    "        if num_files == batch_size:\n",
    "            pass\n",
    "        else:\n",
    "            batch_pixel = load_batches(batch_i)[\"pixels\"]\n",
    "            last_index = batch_pixel.tail(1).index.values[0]\n",
    "            image_path = './Images/Batch'+str(batch_i)+\"/\"\n",
    "\n",
    "            for row_ix in range(last_index+1):\n",
    "                image = batch_pixel.loc[row_ix].split()\n",
    "                image = list(map(int, image))\n",
    "                image = np.array(image).reshape((48,48))\n",
    "                cv2.imwrite(image_path+\"batch_\"+str(batch_i)+\"_\"+str(row_ix)+\".png\", image)\n",
    "\n",
    "    print(\"Checking finished! All images are now available in the path ./Images/\")\n",
    "\n",
    "#::::::::TEST:::::::#\n",
    "training_image_visual(n_batches, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Image Info]\n",
      "\n",
      "Batch: 1 \n",
      "Example of image 1212 with shape of: (48, 48)\n",
      "Label (Emotion): 3\n",
      "\n",
      "[Pixel Info]\n",
      "\n",
      "First 18 pixels: [155, 153, 154, 153, 154, 154, 154, 154, 153, 153, 154, 154, 154, 161, 115, 66, 64, 67] \n",
      "Max value of pixels: 235 \n",
      "MIn value of pixels: 0\n",
      "Type of each pixel: uint8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCvHCN6ZC/gst5ilhQj5Q8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2160009f5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48)\n",
      "[[155 153 154 ..., 153 153 155]\n",
      " [154 153 153 ..., 154 155 155]\n",
      " [154 153 153 ..., 154 155 155]\n",
      " ..., \n",
      " [163 169 166 ..., 208 208 183]\n",
      " [164 170 150 ..., 209 152 111]\n",
      " [165 164 142 ..., 133  92 101]]\n",
      "Picture saved?\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "batch_num = 1     # the batch number (tunable)\n",
    "eximg_ix = 1212    # the index of sample image to be displayed (tunable), max # 5000\n",
    "\n",
    "tmp_data = pd.read_csv(\"training_batch_\"+str(batch_num)+\".csv\")\n",
    "image = tmp_data.loc[eximg_ix]['pixels'].split()\n",
    "img = list(map(int, image))\n",
    "image = np.array(img).astype(np.uint8).reshape((48,48))\n",
    "\n",
    "\n",
    "print (\"[Image Info]\\n\")\n",
    "print (\"Batch:\", batch_num, \"\\nExample of image\", eximg_ix, \"with shape of:\", image.shape)\n",
    "sample_emotion = tmp_data.loc[eximg_ix]['emotion']\n",
    "print (\"Label (Emotion):\", sample_emotion)\n",
    "print (\"\\n[Pixel Info]\\n\")\n",
    "print (\"First 18 pixels:\", img[:18], \"\\nMax value of pixels:\", max(img), \"\\nMIn value of pixels:\", min(img))\n",
    "print (\"Type of each pixel:\", image.dtype)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "print(image.shape)\n",
    "print(image)\n",
    "print(\"Picture saved?\\n\",cv2.imwrite(\"sample.png\", image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def biggest_obj(object):\n",
    "\n",
    "        if object is tuple():\n",
    "            return None, None, None, None\n",
    "\n",
    "        biggest_obj = [0]*5    # biggest_obj = [area, x, y, w, h]\n",
    "\n",
    "        for (x, y, w, h) in object:\n",
    "            biggest_obj.append(w*h)\n",
    "            if w*h > biggest_obj[0]:\n",
    "                biggest_obj[0] = w*h\n",
    "                biggest_obj[1:5] = x, y, w, h\n",
    "\n",
    "        return biggest_obj[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head was successfully detected!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/g9QYwsQb5xcYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21600403fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def detect_head(image):\n",
    "    \n",
    "    init_height, init_width = image.shape[:2]\n",
    "    \n",
    "    face_casc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    faces = face_casc.detectMultiScale(image, scaleFactor=1.1, minNeighbors=3)\n",
    "  \n",
    "    x, y, w, h = biggest_obj(faces)\n",
    "    if x is None:\n",
    "        return False, \"Head was not detected.\", image, init_height, init_width\n",
    "    \n",
    "    image = image[y:y+h,x:x+w]\n",
    "    \n",
    "    return True, \"Head was successfully detected!\", image, init_height, init_width\n",
    "\n",
    "#::::::::TEST:::::::#\n",
    "head_bool, comments, image_arr, init_height, init_width = detect_head(image)\n",
    "print(comments)\n",
    "plt.imshow(image_arr, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_image(image, init_height, init_width):\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # expand in \"width\" direction\n",
    "    append_width = np.zeros( (height, (init_width - width)//2), dtype=np.int8 )\n",
    "    image = np.append(append_width, image, axis=1)\n",
    "    image = np.append(image, append_width, axis=1)\n",
    "\n",
    "    if image.shape[0]%2 != 0:\n",
    "        image = np.append(image, np.zeros((height,1), dtype=np.int8), axis=1)\n",
    "\n",
    "    # expand in \"height direction\n",
    "    append_height = np.zeros( ((init_height - height)//2, init_width), dtype=np.int8 )\n",
    "    image = np.append(append_height, image, axis=0)\n",
    "    image = np.append(image, append_height, axis=0)\n",
    "\n",
    "    if image.shape[0]%2 != 0:\n",
    "        image = np.append(image, np.zeros((1,init_width), dtype=np.int8), axis=0)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of normalized image: (48, 48)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4BMFxMAnCoiBTxQQA58ooP8HRKH5x/n4FeUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2160089ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def norm_image(image, head_bool, init_height, init_width):\n",
    "    \n",
    "    image = ( image - image.mean() )*(100/255)\n",
    "    image = ( image - image.mean(axis=1).reshape(-1,1) )/ image.std(axis=1).reshape(-1,1)\n",
    "    \n",
    "    if head_bool is True:\n",
    "        image = expand_image(image, init_height, init_width)\n",
    "        return image\n",
    "        \n",
    "    return image\n",
    "\n",
    "#::::::::TEST:::::::#\n",
    "normed_image = norm_image(image_arr, head_bool, init_height, init_width)\n",
    "print(\"The shape of normalized image:\",normed_image.shape)\n",
    "plt.imshow(normed_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "<class 'numpy.ndarray'> int8\n",
      "The shape of labels_one_hot: (5000, 7)\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(data):\n",
    "    \n",
    "    \n",
    "    all_labels = data[\"emotion\"].values.ravel()\n",
    "    num_labels = all_labels.shape[0]\n",
    "    \n",
    "    n_classes = len(data.drop_duplicates('emotion')['emotion'].values)\n",
    "    \n",
    "    index_offset = np.arange(num_labels) * n_classes\n",
    "    labels_one_hot = np.zeros((num_labels, n_classes), dtype=np.int8)\n",
    "    labels_one_hot.flat[index_offset + all_labels] = 1\n",
    "    np.array(labels_one_hot).astype(np.uint8)\n",
    "\n",
    "    return labels_one_hot\n",
    "\n",
    "#::::::::TEST:::::::#\n",
    "tmp_data = load_batches(1)\n",
    "labels_one_hot = one_hot_encode(tmp_data)\n",
    "print(labels_one_hot)\n",
    "print(type(labels_one_hot), labels_one_hot.dtype)\n",
    "print(\"The shape of labels_one_hot:\", labels_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\xiaow\\python35\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixels preprocess was successful with the shape of (5000, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "def pixels_preprocess(data):\n",
    "\n",
    "    last_index = data.tail(1).index.values[0]\n",
    "    for row_ix in range(last_index+1):\n",
    "            \n",
    "        each_image = data.loc[row_ix]['pixels'].split()\n",
    "        each_image = np.array(each_image).astype(np.uint8).reshape((48,48))\n",
    "        \n",
    "        # head detection and image normalization\n",
    "        head_bool, comments, image_arr, init_height, init_width = detect_head(each_image)\n",
    "        normed_image = norm_image(image_arr, head_bool, init_height, init_width)\n",
    "        normed_image = normed_image.reshape((1,-1))\n",
    "        \n",
    "        if row_ix == 0:\n",
    "            preprocessed_pixel = normed_image\n",
    "\n",
    "        else:\n",
    "            preprocessed_pixel = np.append(preprocessed_pixel, normed_image, axis=0)\n",
    "        \n",
    "    return preprocessed_pixel.reshape((-1, 48, 48, 1))\n",
    "\n",
    "#::::::::TEST:::::::#\n",
    "tmp_data = load_batches(1)\n",
    "preprocessed_pixel = pixels_preprocess(tmp_data)\n",
    "print(\"pixels preprocess was successful with the shape of\", preprocessed_pixel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\xiaow\\python35\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training images: (4500, 48, 48, 1)\n",
      "The shape of validation images: (500, 48, 48, 1)\n",
      "The shape of training labels: (4500, 7)\n",
      "The shape of validation labels: (500, 7)\n"
     ]
    }
   ],
   "source": [
    "def train_valid_split(data):\n",
    "    \n",
    "    images = pixels_preprocess(data)\n",
    "    labels = one_hot_encode(data)\n",
    "\n",
    "    train_image, valid_image, train_label, valid_label = train_test_split(images, labels,\n",
    "                                                                          stratify=labels, \n",
    "                                                                          test_size=0.1, \n",
    "                                                                          random_state = 42)\n",
    "\n",
    "    return train_image, valid_image, train_label, valid_label\n",
    "\n",
    "#::::::::TEST:::::::#\n",
    "batch_data = load_batches(1)\n",
    "train_image, valid_image, train_label, valid_label = train_valid_split(batch_data)\n",
    "\n",
    "print(\"The shape of training images:\", train_image.shape)\n",
    "print(\"The shape of validation images:\", valid_image.shape)\n",
    "print(\"The shape of training labels:\", train_label.shape)\n",
    "print(\"The shape of validation labels:\", valid_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\xiaow\\python35\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'validation_data.p' was saved successfully\n"
     ]
    }
   ],
   "source": [
    "def train_valid_data_save(n_batches):\n",
    "    \n",
    "    for batch_i in range(1,n_batches+1):\n",
    "        \n",
    "        batch_split_file = Path(\"C:/Users/xiaow/Desktop/fer2013/batch_\"+str(batch_i)+\"_split.p\")\n",
    "        if batch_split_file.is_file():\n",
    "            print(\"File 'batch_\"+str(batch_i)+\"_split.p' is found\")\n",
    "            \n",
    "        else:\n",
    "               \n",
    "            batch_data = load_batches(batch_i)\n",
    "            train_image_i, valid_image_i, train_label_i, valid_label_i = train_valid_split(batch_data)\n",
    "            pickle.dump((train_image_i, train_label_i), open(\"batch_\"+str(batch_i)+\"_training_data.p\", 'wb'))\n",
    "            \n",
    "        if batch_i == 1:\n",
    "            valid_image, valid_label = valid_image_i, valid_label_i\n",
    "\n",
    "        else:\n",
    "            valid_image = np.append(valid_image, valid_image_i, axis=0)\n",
    "            valid_label = np.append(valid_label, valid_label_i, axis=0)\n",
    "        \n",
    "    pickle.dump((valid_image, valid_label), open(\"validation_data.p\", 'wb'))\n",
    "    print(\"File 'validation_data.p' was saved successfully\")\n",
    "        \n",
    "    return None\n",
    "\n",
    "#::::::::TEST::::::#\n",
    "n_batches = 6\n",
    "train_valid_data_save(n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_images, valid_labels = pickle.load(open(\"validation_data.p\", mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    \n",
    "    image_shape = list(image_shape)\n",
    "    return tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], image_shape[2]], name=\"x\")\n",
    "\n",
    "def nn_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y\")\n",
    "\n",
    "def nn_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.placeholder(tf.float32, shape=None, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x_tensor, conv_num_outputs, conv_ksize, conv_strides, padding):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param padding: padding method ('VALID' or 'SAME')\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, image_height, image_width, channels = x_tensor.get_shape().as_list()\n",
    "                     \n",
    "    conv_layer = tf.nn.conv2d(x_tensor,\n",
    "                              filter=tf.Variable(tf.truncated_normal(list(conv_ksize)+[channels, conv_num_outputs])),\n",
    "                              strides=[1]+list(conv_strides)+[1],\n",
    "                              padding=padding\n",
    "                              )\n",
    "\n",
    "    conv_layer = tf.nn.bias_add(value=conv_layer,\n",
    "                                bias=tf.Variable(tf.zeros(conv_num_outputs))\n",
    "                                )\n",
    "    \n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "   \n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxpooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(x_tensor, pool_ksize, pool_strides, padding):\n",
    "\n",
    "    maxpool_layer = tf.nn.max_pool(x_tensor,\n",
    "                                   ksize=[1]+list(pool_ksize)+[1],\n",
    "                                   strides=[1]+list(pool_strides)+[1],\n",
    "                                   padding=padding\n",
    "                                   )\n",
    "    \n",
    "    return maxpool_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    batch_size, image_height, image_width, channels = x_tensor.get_shape().as_list()\n",
    "    flatten_layer = tf.reshape(x_tensor,\n",
    "                               shape=[-1, image_height*image_width*channels]\n",
    "                               )\n",
    "    \n",
    "    return flatten_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, other = x_tensor.get_shape().as_list()\n",
    "\n",
    "    weight = tf.Variable( tf.truncated_normal(shape = [other, num_outputs]) )\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "\n",
    "    full_layer = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    full_layer = tf.nn.relu(full_layer)\n",
    "    \n",
    "    return full_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    batch_size, other = x_tensor.get_shape().as_list()\n",
    "\n",
    "    weight = tf.Variable( tf.truncated_normal(shape = [other, num_outputs]) )\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "       \n",
    "    output_layer = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_nn = conv2d(x, conv_num_outputs=96, conv_ksize=(5,5), conv_strides=(2,2), padding='SAME')\n",
    "    \n",
    "    conv_nn = max_pool(conv_nn, pool_ksize=(3,3), pool_strides=(2,2), padding='SAME')\n",
    "       \n",
    "    conv_nn = conv2d(conv_nn, conv_num_outputs=128, conv_ksize=(3,3), conv_strides=(2,2), padding='SAME')\n",
    "    \n",
    "    conv_nn = max_pool(conv_nn, pool_ksize=(3,3), pool_strides=(2,2), padding='SAME')\n",
    "    \n",
    "    conv_nn = tf.nn.dropout(conv_nn, keep_prob)\n",
    "    \n",
    "    conv_nn = conv2d(conv_nn, conv_num_outputs=256, conv_ksize=(3,3), conv_strides=(2,2), padding='SAME')\n",
    "\n",
    "    conv_nn = max_pool(conv_nn, pool_ksize=(3,3), pool_strides=(2,2), padding='SAME')                         \n",
    "  \n",
    "    conv_nn = flatten(conv_nn)\n",
    "    \n",
    "    conv_nn = fully_conn(conv_nn, num_outputs = 2000)\n",
    "\n",
    "    conv_nn = fully_conn(conv_nn, num_outputs = 2000)\n",
    "    \n",
    "    conv_nn = output(conv_nn, num_outputs = 7)\n",
    "    \n",
    "    return conv_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import problem_unittests as tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = nn_image_input((48,48,1))\n",
    "y = nn_label_input(7)\n",
    "keep_prob = nn_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "print(cost)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, train_images, train_labels):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : train_images: Batch of Numpy image data\n",
    "    : train_labels: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "\n",
    "    session.run(optimizer, feed_dict={x:train_images, y:train_labels, keep_prob:keep_probability})\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, train_images, train_labels, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : train_images: Batch of Numpy image data\n",
    "    : train_labels: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "\n",
    "    loss = session.run(cost, feed_dict={x:train_images, y:train_labels, keep_prob:1.0})\n",
    "    print(cost,\"cost\")\n",
    "    valid_accuracy = sess.run(accuracy, feed_dict={\n",
    "                x: valid_images,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.0})\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_accuracy))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, Batch 1:  Tensor(\"Mean:0\", shape=(), dtype=float32) cost\n",
      "Loss:        nan Validation Accuracy: 0.136886\n",
      "Epoch  2, Batch 1:  Tensor(\"Mean:0\", shape=(), dtype=float32) cost\n",
      "Loss:        nan Validation Accuracy: 0.136886\n",
      "Epoch  3, Batch 1:  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-24d4d9e05ced>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch {:>2}, Batch {}:  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-69-9fe8c62658f0>\u001b[0m in \u001b[0;36mprint_stats\u001b[1;34m(session, train_images, train_labels, cost, accuracy)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"cost\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     valid_accuracy = sess.run(accuracy, feed_dict={\n",
      "\u001b[1;32mc:\\users\\xiaow\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xiaow\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xiaow\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\users\\xiaow\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xiaow\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Checking the Training on a Single Batch...')\n",
    "\n",
    "batch_i = 1\n",
    "batch_step = 128\n",
    "epochs = 60\n",
    "keep_probability = 0.8\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    count = 0\n",
    "    train_images, train_labels = pickle.load(open(\"batch_\"+str(batch_i)+\"_training_data.p\", mode='rb'))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for step in range(len(train_images)//batch_step + 1):\n",
    "            train_neural_network(sess, optimizer, keep_probability, \n",
    "                                 train_images[count:count+batch_step], train_labels[count:count+batch_step])\n",
    "            if count+batch_step > len(train_images):\n",
    "                count += len(train_images)-count\n",
    "            else:\n",
    "                count += batch_step\n",
    "\n",
    "        print('Epoch {:>2}, Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, train_images, train_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
